---
layout: post
title: Web Scraping and Linear Regression Modeling
---

# Predicting Annual Earnings on the PGA Tour

For my second project of Metis Data Science Bootcamp I was tasked with building a linear regression model from information scraped from the web.  As an amateur golfer, I was interested in investigating which part of a golfer's game matters the most, and thought that looking at professional golfers' statistics would be a great place to start.



## Data and Method

In addition to exploring whether there is any truth behind the saying "Drive for dough, put for show", I wanted to know if a golfer's physical attributes had any impact on his/her game.  My primary sources of data came from the Official PGA website, as well as height weight and age data from Newsday.  I scraped the data by using the BeautifulSoup python library, cleaned the data using pandas and numpy, and finally combined my cleaned data with a dataset found on Kaggle for a total of 1435 observations.  Below is the code I used for scraping and cleaning:

```python
import requests
from bs4 import BeautifulSoup
import numpy as np
import pandas as pd
import time
import random
import string
from dateutil import parser
from datetime import datetime

alph = string.ascii_uppercase
players = {}
for letter in alph:
	url = 'https://newsday.sportsdirectinc.com/golf/pga-players.aspx?page=/data/pga/players/'\
	+letter+'_players.html'
	response = requests.get(url)
	page = response.text
	soup = BeautifulSoup(page, 'html5')
	tables = soup.find_all('table')
	rows = [row for row in tables[1].find_all('tr')]
	rows = rows[1:]
	for row in rows:
	    items = row.find_all('td')
	    player = items[0].find('a')['href']
	    players[player] = [i.text for i in items [0:]]
	time.sleep(.5+2*random.random())
	print(letter)

df = pd.DataFrame.from_dict(players).transpose()
df = df.reset_index().drop('index', axis=1)
df = df.apply(lambda x: x.str.strip())
df = df.apply(lambda x: x.replace('', np.nan))
df = df.dropna().reset_index().drop('index', axis=1)
df['name'] = df[0].str.split(', ').str[::-1].str.join(' ')

def get_inches(height):
    ht = height.split("'")
    feet = float(ht[0])
    inches = float(ht[1].replace("\"",""))
    return (12*feet) + inches

df['height_inches'] = df[1].apply(lambda x:get_inches(x))
df['weight_lbs'] = df[2].str.split(' ').str[0]
df['birthyear'] = df[3].apply(lambda x:parser.parse(x).year)        

def convert_year(year):
    if year > 2000:
        return year-100
    else:
        return year

df['birthyear'] = df['birthyear'].apply(convert_year)
df = df.drop([0,1,2,3,4], axis=1)
d = pd.read_csv('pgaTourData.csv')
d = d.rename(index=str, columns={'Player Name': 'name'})
new_df = pd.merge(df, d, on='name')
new_df = new_df.drop('Wins', axis=1)
new_df['Top 10'] = new_df['Top 10'].fillna(0)
new_df = new_df.dropna().reset_index().drop('index', axis=1)

years = range(2010, 2019)
d = {}
for year in years:
	url = 'https://www.pgatour.com/stats/stat.109.'\
	+str(year)+'.html'
	response = requests.get(url)
	page = response.text
	soup = BeautifulSoup(page, 'html5')
	tables = soup.find_all('table')
	rows = [row for row in tables[1].find_all('tr')]
	rows = rows[1:]
	players = {}
	for row in rows:
	    items = row.find_all('td')
	    player = items[2].find('a')['href']
	    players[player] = [items[2].text, items[4].text, year]
	d[year] = pd.DataFrame.from_dict(players).transpose()
	time.sleep(1+2*random.random())
	print(year)

mdf = pd.concat(d.values(), ignore_index=True)

def money_to_int(mon):
    mon = mon.replace('$','').replace(',','')
    return int(mon)

mdf['name'] = mdf[0].apply(lambda x: x.strip())
mdf['money'] = mdf[1].apply(lambda x: money_to_int(x))
mdf = mdf.drop([0,1], axis=1)
mdf = mdf.rename(index=int, columns={2: 'Year'})
g = pd.merge(new_df, mdf, on=['name','Year'])
g = g.drop('Money', axis=1)
cols = g.columns.tolist()
cols.insert(0, cols.pop(cols.index('money')))
g = g.reindex(columns=cols)
g['weight_lbs'] = g['weight_lbs'].apply(int)
g['Year'] = g['Year'].apply(int)
g['Points'] = g['Points'].apply(lambda x: x.replace(',', '')).apply(int)
```

Since golf rankings aren't very granular, I thought it would be much more interesting to create a model to predict annual earnings for a player on the PGA Tour.  However the distribution was very much right skewed, making it difficult to model with linear relationships.  A log transformation helped with this, and made the distribution much more normal-looking.

![]({{ site.url}}/images/earnings.png)

![]({{ site.url}}/images/logearnings.png)



## Analysis and Plots

Now for the fun part.  In deciding which features to include in my model, I felt that it was best to not include much more obvious predictors of tournament earnings such as average score, wins, and top 10 finishes.  In addition I wanted to run models separately on traditional statistics (fairway percentage, average drive distance, greens in regulation, average scrambling, and average putts) vs strokes gained statistics (SG off-the-tee, SG approach-the-green, SG around-the-green, and SG putting), and finally a model that includes both traditional and SG stats.  Through cross-validation I ran simple linear regression, lasso, ridge, and polynomial regression on all three combinations of features, and ultimately found that simple linear regression produced the highest R-squared and lowest mean squared error for traditional statistics alone and SG statistics alone.  When combining all features, a lasso on a degree 2 polynomial regression performed the best, with an R-squared of 0.765 on my holdout set, and a root mean-squared-error of $740k.  Below is my code for initial cross validation testing, so that I could compare the R-squared and rmse of different models:

```python
import pandas as pd
import numpy as np
import pickle
import seaborn as sns
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.linear_model import LinearRegression, Ridge, Lasso, LassoCV
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
import math
with open('pga.pickle', 'rb') as file:
    g = pickle.load(file)
#Log Transform on 'money'
g['logmoney'] = g['money'].apply(lambda x: math.log(x))
#For All minus obvious features
g_small = g.drop(['Average Score', 'Points', 'Top 10', 'Average SG Total', 'Year','name'], axis=1)
#For All minus obvious features AND minus more features based on lasso and ridge
g_smaller = g.drop(['Avg Distance','height_inches', 'weight_lbs', 'Fairway Percentage','Average Score', 'Points', 'Top 10', 'Average SG Total', 'Year','name'], axis=1)
#no SG features
g_noSG = g.drop(['Average Score', 'Points', 'Top 10', 'Average SG Total', 'Year','name', 'Average SG Putts', 'SG:OTT', 'SG:APR','SG:ARG'], axis=1)
#only SG features
g_onlySG = g[['logmoney','money','Rounds','Average SG Putts','SG:OTT','SG:APR','SG:ARG']]
#SG without rounds
g_pureSG = g[['logmoney','money','Average SG Putts','SG:OTT','SG:APR','SG:ARG']]

#CV Test with errors Ridge SCALED and poly 2
def cv_errors(df, r_alpha):
    X, y = df.drop(['logmoney','money'], axis=1), df['logmoney']
    X, X_test, y, y_test = train_test_split(X, y, test_size=.2, random_state=1)
    X, y = np.array(X), np.array(y)
    kf = KFold(n_splits=5, shuffle=True, random_state = 10)
    cv_lm_r2s, cv_lm_reg_r2s, cv_poly_r2s = [], [], []
    lm_train, lm_reg_train, poly_train = [], [], []
    for train_ind, val_ind in kf.split(X,y):
        X_train, y_train = X[train_ind], y[train_ind]
        X_val, y_val = X[val_ind], y[val_ind] 
        #simple linear regression
        lm = LinearRegression()
        lm.fit(X_train, y_train)
        cv_lm_r2s.append(lm.score(X_val, y_val))
        lm_train.append(lm.score(X_train, y_train))
        #ridge with feature scaling
        lm_reg = Ridge(alpha=r_alpha)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_val_scaled = scaler.transform(X_val)
        lm_reg.fit(X_train_scaled, y_train)
        cv_lm_reg_r2s.append(lm_reg.score(X_val_scaled, y_val))
        lm_reg_train.append(lm_reg.score(X_train_scaled, y_train))
        #polynomial regression with degree 2
        poly = PolynomialFeatures(degree=2)
        X_train_poly = poly.fit_transform(X_train)
        X_val_poly = poly.transform(X_val)
        lm_poly = LinearRegression()
        lm_poly.fit(X_train_poly, y_train)
        cv_poly_r2s.append(lm_poly.score(X_val_poly, y_val))
        poly_train.append(lm_poly.score(X_train_poly, y_train))
    print(f'Simple mean train R^2: {np.mean(lm_train):.3f} +- {np.std(lm_train):.3f}')
    print(f'Simple mean cv R^2: {np.mean(cv_lm_r2s):.3f} +- {np.std(cv_lm_r2s):.3f}')
    print('Simple rmse: ', math.sqrt(mean_squared_error(y_val, lm.predict(X_val))))
    print(f'Ridge scaled mean train R^2: {np.mean(lm_reg_train):.3f} +- {np.std(lm_reg_train):.3f}')
    print(f'Ridge scaled mean cv R^2: {np.mean(cv_lm_reg_r2s):.3f} +- {np.std(cv_lm_reg_r2s):.3f}')
    print('Ridge scaled rmse: ', math.sqrt(mean_squared_error(y_val, lm_reg.predict(X_val_scaled))))
    print(f'Degree 2 polynomial regression train R^2: {np.mean(poly_train):.3f} +- {np.std(poly_train):.3f}')
    print(f'Degree 2 polynomial regression cv R^2: {np.mean(cv_poly_r2s):.3f} +- {np.std(cv_poly_r2s):.3f}')
    print('Degree 2 polynomial regression rmse: ', math.sqrt(mean_squared_error(y_val, lm_poly.predict(X_val_poly))))
```

For a more concrete example on how my model performed on an individual player, I ran the model for Jordan Spieth and compared the predicted versus actual earnings:

![]({{ site.url}}/images/results.png)

## Conclusion

When running a simple linear model on only traditional stats, average putts was the most predictive feature, with average driving distance and greens in regulation the second most predictive.  As for SG stats, the most predictive features were SG off-the-tee and SG approach-the-green, with SG putting as slightly less predictive.  When combining the metrics in my final model, strokes gained stats had more impact on the predictive value compared to traditional stats.  Based on my analysis, there was no clear winning predictor in any of the models that I ran.  Lastly, it turns out that weight, height, and age had almost no impact on a player's winnings.